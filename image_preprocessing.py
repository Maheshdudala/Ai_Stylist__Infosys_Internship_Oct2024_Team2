# -*- coding: utf-8 -*-
"""Image_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oWi4Nm0Tc2giIRw1tmmWaIN-TnbOFr4
"""

import os
import requests
import pandas as pd
from google.colab import drive
from concurrent.futures import ThreadPoolExecutor

# Step 1: Mount Google Drive
drive.mount('/content/drive')

# Step 2: Read the CSV file that contains image URLs
csv_file_path = 'updated_dataset.csv'  # Update with the path to your CSV file containing image links
df = pd.read_csv(csv_file_path)

# Assuming the column with image URLs is named 'link' and the column with the IDs is named 'id'
image_links = df['link']
image_ids = df['id']

# Destination folder on Google Drive where the images will be saved
destination_folder = '/content/drive/MyDrive/Images'  # Update this to your preferred folder in Google Drive

# Make sure the destination folder exists
os.makedirs(destination_folder, exist_ok=True)

# Function to download a single image
def download_image(link, image_id):
    image_filename = os.path.join(destination_folder, f'{image_id}')

    # Check if the file already exists
    if os.path.exists(image_filename):
        print(f'Skipping {image_filename}, already exists.')
        return

    try:
        # Get the image content
        response = requests.get(link, stream=True)
        response.raise_for_status()  # Check if the request was successful

        # Save the image to the destination folder
        with open(image_filename, 'wb') as file:
            for chunk in response.iter_content(chunk_size=8192):
                file.write(chunk)

        print(f'Successfully downloaded: {image_filename}')
    except requests.HTTPError as http_err:
        print(f'HTTP error occurred: {http_err} - {link}')
    except Exception as err:
        print(f'Other error occurred: {err} - {link}')

# Step 3: Download images concurrently
with ThreadPoolExecutor(max_workers=20) as executor:  # Adjust max_workers as needed
    executor.map(download_image, image_links, image_ids)

print(f"All images have been downloaded to {destination_folder} on your Google Drive.")

import tensorflow as tf
import pandas as pd
import os
from google.colab import drive

# Load the CSV file
csv_file = 'updated_dataset.csv'
data = pd.read_csv(csv_file)
image_folder ='/content/drive/MyDrive/Images'

# Extract the image filenames and labels (using 'articleType' as the label)
image_filenames = data['filename'].values
labels = data['articleType'].values

# Parameters for preprocessing
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Function to load and preprocess the images
def preprocess_image(image_path):
    # Load the image
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)  # Decode as JPEG, 3 channels for RGB

    # Resize the image
    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])

    # Normalize the pixel values to [0, 1]
    image = image / 255.0

    return image

# Function to load image and corresponding label
def load_image_and_label(image_filename, label):
    # Construct the full image path
    image_path = tf.strings.join([image_folder, image_filename], separator=os.sep)
    image = preprocess_image(image_path)
    return image, label

# Create a TensorFlow dataset
dataset = tf.data.Dataset.from_tensor_slices((image_filenames, labels))

# Map the preprocessing function to the dataset
dataset = dataset.map(load_image_and_label, num_parallel_calls=tf.data.AUTOTUNE)

# Shuffle, batch, and prefetch the dataset for performance
dataset = dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)

# Function to display a batch of images
def display_images(batch, labels, batch_size=5):
    plt.figure(figsize=(15, 15))
    for i in range(batch_size):
        ax = plt.subplot(batch_size, 1, i + 1)
        plt.imshow(batch[i])
        plt.title(f"Label: {labels[i].decode('utf-8')}")
        #plt.axis("off")
    plt.show()

import matplotlib.pyplot as plt
# Set batch size for displaying images
display_batch_size = 5  # Change to 3 if you want to display 3 images

# Iterate over the dataset and display images
for images, labels in dataset.take(1):  # Take one batch from the dataset
    display_images(images.numpy(), labels.numpy(), batch_size=display_batch_size)
    break  # Displaying only the first batch

