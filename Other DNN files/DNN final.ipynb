{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n","from sklearn.model_selection import train_test_split\n","\n","data_path = '/content/drive/MyDrive/dataset/Final-Fashion-Dataset.csv'\n","df = pd.read_csv(data_path)"],"metadata":{"id":"07hBby7UOBxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing\n","categorical_features = ['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'usage']\n","numerical_features = ['year', 'Price (USD)']\n","id_features = ['user-id', 'id']"],"metadata":{"id":"EBEq62FxePpw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Encode categorical features\n","for feature in categorical_features:\n","    le = LabelEncoder()\n","    df[feature] = le.fit_transform(df[feature])"],"metadata":{"id":"_lK7uz0UeWFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Scale numerical features\n","scaler = MinMaxScaler()\n","df[numerical_features] = scaler.fit_transform(df[numerical_features])"],"metadata":{"id":"b1Fj4B0beUZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Define input layers\n","categorical_inputs = []\n","for feature in categorical_features:\n","    input_layer = Input(shape=(1,), name=feature)\n","    categorical_inputs.append(input_layer)\n","\n","numerical_input = Input(shape=(len(numerical_features),), name='numerical_features')\n"],"metadata":{"id":"3FlKxpGkeb1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Embedding layers for categorical features\n","embedding_layers = []\n","for input_layer, feature in zip(categorical_inputs, categorical_features):\n","    embedding_layer = Embedding(input_dim=df[feature].nunique(), output_dim=10)(input_layer)  # Adjust output_dim as needed\n","    embedding_layers.append(embedding_layer)"],"metadata":{"id":"3E8e8ZZpefKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. Flatten embedding layers\n","flatten_layers = []\n","for embedding_layer in embedding_layers:\n","    flatten_layer = Flatten()(embedding_layer)\n","    flatten_layers.append(flatten_layer)\n","\n","# 6. Concatenate all layers\n","all_features = Concatenate()([numerical_input] + flatten_layers)"],"metadata":{"id":"LenngyXrejEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. Define dense layers (DNN architecture)\n","x = Dense(128, activation='relu')(all_features)\n","x = Dense(64, activation='relu')(x)\n","output = Dense(1, activation='linear')(x)  # Assuming a regression task (predicting 'Price (USD)')\n","\n","# 8. Create the model\n","model = Model(inputs=[numerical_input] + categorical_inputs, outputs=output)\n","\n","# Compile the model (define optimizer, loss function, metrics)\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"],"metadata":{"id":"BdgKviDleoFI"},"execution_count":null,"outputs":[]},{"source":["# Prepare data for training\n","# 1. Separate features and target variable\n","X = df[['gender', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'season', 'usage', 'year', 'Price (USD)']]\n","y = df['Price (USD)']  # Assuming 'Price (USD)' is the target variable\n","\n","# 2. Create input data for the model\n","X_numerical = X[numerical_features].values\n","# Reshape X_categorical to be a single array with samples as rows and features as columns\n","X_categorical = X[categorical_features].values\n","\n","# 3. Split data into training and testing sets\n","X_train_num, X_test_num, X_train_cat, X_test_cat, y_train, y_test = train_test_split(\n","    X_numerical, X_categorical, y, test_size=0.2, random_state=42\n",")\n","\n","# Train the model\n","model.fit(\n","    x=[X_train_num] + [X_train_cat[:, i] for i in range(X_train_cat.shape[1])],  # Input data - list of arrays, one for each input layer\n","    y=y_train,  # Target variable\n","    epochs=10,  # Number of training epochs\n","    batch_size=32,  # Batch size\n","    validation_data=([X_test_num] + [X_test_cat[:, i] for i in range(X_test_cat.shape[1])], y_test)  # Validation data - list of arrays, one for each input layer\n",")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWXNpBzofcx4","executionInfo":{"status":"ok","timestamp":1732524286015,"user_tz":-330,"elapsed":49009,"user":{"displayName":"Akshara K","userId":"18379555469296361804"}},"outputId":"26ae5d7b-5195-40e1-ffca-a557e3e4e3e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0067 - mae: 0.0276 - val_loss: 4.0164e-06 - val_mae: 0.0015\n","Epoch 2/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 3.7497e-06 - mae: 0.0015 - val_loss: 1.7113e-06 - val_mae: 9.7268e-04\n","Epoch 3/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.4604e-06 - mae: 0.0012 - val_loss: 1.8266e-06 - val_mae: 0.0011\n","Epoch 4/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.3113e-06 - mae: 0.0012 - val_loss: 8.9293e-07 - val_mae: 6.8860e-04\n","Epoch 5/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 3.0101e-06 - mae: 0.0013 - val_loss: 2.7724e-06 - val_mae: 0.0014\n","Epoch 6/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 6.2313e-06 - mae: 0.0019 - val_loss: 5.0689e-06 - val_mae: 0.0021\n","Epoch 7/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.0428e-06 - mae: 0.0011 - val_loss: 3.9514e-04 - val_mae: 0.0197\n","Epoch 8/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 1.5165e-05 - mae: 0.0019 - val_loss: 3.4388e-06 - val_mae: 0.0018\n","Epoch 9/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2.8084e-06 - mae: 0.0012 - val_loss: 1.1887e-07 - val_mae: 2.2456e-04\n","Epoch 10/10\n","\u001b[1m913/913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2.0201e-06 - mae: 9.2287e-04 - val_loss: 2.7614e-07 - val_mae: 4.0455e-04\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7a19c89f7b20>"]},"metadata":{},"execution_count":71}]},{"source":["def recommend_items_for_user(user_id, top_k=5):\n","    \"\"\"Recommends top_k items for a given user_id.\n","\n","    Args:\n","        user_id: The ID of the user.\n","        top_k: The number of items to recommend.\n","\n","    Returns:\n","        A list of item IDs representing the recommendations.\n","    \"\"\"\n","\n","    # 1. Filter items not yet interacted with by the user (if applicable)\n","    # Assuming you have an interaction history, you can filter items the user has already interacted with.\n","    # This might require modifications based on your data structure.\n","\n","    all_items = df['id'].unique()  # Example: Get all unique item IDs\n","    # ... (filter out items already interacted with) ...\n","\n","    # 2. Create input data for the model for each candidate item\n","    # For simplicity, we'll predict the price for all items and then rank them based on the prediction\n","\n","    input_data_num = []\n","    input_data_cat = [[] for _ in range(len(categorical_features))]\n","\n","    for item_id in all_items:\n","        item_data = df[df['id'] == item_id].iloc[0]  # Get data for the item\n","\n","        # Extract numerical features\n","        input_data_num.append(item_data[numerical_features].values.astype('float32'))\n","\n","        # Extract categorical features\n","        for i, feature in enumerate(categorical_features):\n","            input_data_cat[i].append(item_data[feature])\n","\n","    input_data_num = np.array(input_data_num)\n","    input_data_cat = [np.array(cat_data) for cat_data in input_data_cat]\n","\n","    # 3. Predict prices for all items\n","    predicted_prices = model.predict([input_data_num] + input_data_cat)\n","\n","    # 4. Rank items by predicted price (or another relevant metric)\n","    item_scores = predicted_prices.flatten()  # Assuming lower prices are preferred\n","\n","    # 5. Get the top_k items\n","    top_item_indices = np.argsort(item_scores)[:top_k]  # Sort in ascending order for price\n","    recommended_item_ids = all_items[top_item_indices]\n","\n","    return recommended_item_ids\n","\n","# Example usage:\n","user_id = 36062  # Replace with a user ID\n","recommendations = recommend_items_for_user(user_id)\n","print(f\"Recommendations for user {user_id}: {recommendations}\")"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwwakw_ugv95","outputId":"fd705e22-3fba-4077-dc44-ace7f03fefbe","executionInfo":{"status":"ok","timestamp":1732525087100,"user_tz":-330,"elapsed":48120,"user":{"displayName":"Akshara K","userId":"18379555469296361804"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1141/1141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n","Recommendations for user 36062: [13727 15371  1789 13146 41669]\n"]}]}]}